{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDENTITY_ATTACK</th>\n",
       "      <th>INSULT</th>\n",
       "      <th>PROFANITY</th>\n",
       "      <th>SEVERE_TOXICITY</th>\n",
       "      <th>THREAT</th>\n",
       "      <th>TOXICITY</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>message_comments</th>\n",
       "      <th>mainTopic</th>\n",
       "      <th>message_posts</th>\n",
       "      <th>secondTopic</th>\n",
       "      <th>shares</th>\n",
       "      <th>title</th>\n",
       "      <th>time_difference</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102216</td>\n",
       "      <td>0.651296</td>\n",
       "      <td>0.664565</td>\n",
       "      <td>0.350583</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.588517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Faux, ce ne sera jamais le temps pour lui car ...</td>\n",
       "      <td>composer-preview</td>\n",
       "      <td>HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...</td>\n",
       "      <td>7FJ4TUHKEFEXFIZI6DY2WAQE4E</td>\n",
       "      <td>0</td>\n",
       "      <td>Les incohérences de l’ingérence</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.167515</td>\n",
       "      <td>0.255499</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>0.176317</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon dieu!! On nous prend sérieusement pour des...</td>\n",
       "      <td>composer-preview</td>\n",
       "      <td>HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...</td>\n",
       "      <td>7FJ4TUHKEFEXFIZI6DY2WAQE4E</td>\n",
       "      <td>0</td>\n",
       "      <td>Les incohérences de l’ingérence</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDENTITY_ATTACK    INSULT  PROFANITY  SEVERE_TOXICITY    THREAT  TOXICITY  \\\n",
       "0         0.102216  0.651296   0.664565         0.350583  0.029933  0.588517   \n",
       "1         0.005476  0.167515   0.255499         0.006981  0.005647  0.176317   \n",
       "\n",
       "   comment_count  like_count  \\\n",
       "0              0           0   \n",
       "1              1           1   \n",
       "\n",
       "                                    message_comments         mainTopic  \\\n",
       "0  Faux, ce ne sera jamais le temps pour lui car ...  composer-preview   \n",
       "1  Mon dieu!! On nous prend sérieusement pour des...  composer-preview   \n",
       "\n",
       "                                       message_posts  \\\n",
       "0  HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...   \n",
       "1  HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...   \n",
       "\n",
       "                  secondTopic  shares                            title  \\\n",
       "0  7FJ4TUHKEFEXFIZI6DY2WAQE4E       0  Les incohérences de l’ingérence   \n",
       "1  7FJ4TUHKEFEXFIZI6DY2WAQE4E       0  Les incohérences de l’ingérence   \n",
       "\n",
       "   time_difference  year  month  weekday  \n",
       "0              3.0  2023      6        4  \n",
       "1             18.0  2023      6        4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== Importer les library ====== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ====== Déterminer les path ====== #\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = os.path.dirname(cwd)\n",
    "data_path = parent + '\\\\data\\\\'\n",
    "\n",
    "df =pd.read_hdf(cwd + '\\\\dataframe\\\\' + 'df_clean.h5')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(data_path + 'Posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_test = posts[['title']].head(1000).copy().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classification des titres avec cammemBert\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Assuming you have a dataframe df with a column 'title' for news titles\n",
    "# and a list topics with your topics\n",
    "topics = ['actualités', 'politique', 'sports', 'divertissement', 'santé', 'éducation',\n",
    "                         'hockey', 'musique', 'auto', 'affaires-locales', 'environnement', 'covid-19',\n",
    "                         'science', 'voyages', 'mode', 'arts', 'alimentation', 'cinéma', 'technologie',\n",
    "                         'culture', 'jeunesse', 'expositions', 'techno', 'théâtre', 'livres', 'voyages',\n",
    "                         'recettes', 'arts-visuels', 'sexe', 'estrie', 'remparts', 'sciences', 'société',\n",
    "                         'oceanic', 'vin', 'guerre', 'canadiens', 'celebrites']\n",
    "\n",
    "# topics = ['covid', 'politique', 'art', 'sport', 'environnement', 'autre']\n",
    "df = post_test # your dataframe with news titles\n",
    "\n",
    "# Load the BERT tokenizer and BERT model\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels=len(topics),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Tokenize all titles in the dataframe\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for title in df['title']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        title,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=64,\n",
    "                        pad_to_max_length=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists into tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks)\n",
    "dataloader = DataLoader(\n",
    "            dataset,\n",
    "            sampler=SequentialSampler(dataset),\n",
    "            batch_size=32\n",
    ")\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8\n",
    "                )\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Assume a multi-class classification problem\n",
    "total_steps = len(dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# Implement training loop, evaluation, etc.\n",
    "\n",
    "# Save your model\n",
    "model.save_pretrained('path_to_save_model')\n",
    "\n",
    "# Load the model for inference\n",
    "model = CamembertForSequenceClassification.from_pretrained('path_to_save_model')\n",
    "\n",
    "# Predict function\n",
    "def predict(title, model=model):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        title,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=64,\n",
    "                        pad_to_max_length=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_id, token_type_ids=None, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    index = logits.argmax()\n",
    "    return topics[index]\n",
    "\n",
    "# Classify all titles\n",
    "df['predicted_topic'] = df['title'].apply(predict)\n",
    "df.to_excel('TopicsClassification.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Apiori\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(data_path + 'Posts.csv')\n",
    "post_test = posts[['title']].head(100).copy().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Une électrification à « la mesure des besoins de remplacement »'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_test['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une électrification à « la mesure des besoins de remplacement »\n",
      "électrification « mesure besoins remplacement »\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "text = post_test['title'][0]\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "clean_text = ' '.join(filtered_words)\n",
    "\n",
    "print(text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy_cleaner' has no attribute 'Pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy_cleaner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m removers, replacers, mutators\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_core_news_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_cleaner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m(\n\u001b[0;32m      7\u001b[0m     model,\n\u001b[0;32m      8\u001b[0m     removers\u001b[38;5;241m.\u001b[39mremove_stopword_token,\n\u001b[0;32m      9\u001b[0m     replacers\u001b[38;5;241m.\u001b[39mreplace_punctuation_token,\n\u001b[0;32m     10\u001b[0m     mutators\u001b[38;5;241m.\u001b[39mmutate_lemma_token,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m text \u001b[38;5;241m=\u001b[39m post_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m text_clean \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mclean(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy_cleaner' has no attribute 'Pipeline'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy_cleaner\n",
    "from spacy_cleaner.processing import removers, replacers, mutators\n",
    "\n",
    "model = spacy.load(\"fr_core_news_sm\")\n",
    "pipeline = spacy_cleaner.Pipeline(\n",
    "    model,\n",
    "    removers.remove_stopword_token,\n",
    "    replacers.replace_punctuation_token,\n",
    "    mutators.mutate_lemma_token,\n",
    ")\n",
    "\n",
    "text = post_test['title'][0]\n",
    "\n",
    "text_clean = pipeline.clean(text)\n",
    "\n",
    "print(text)\n",
    "print(text_clean)\n",
    "\n",
    "# ['hello _IS_PUNCT_ Cellan _IS_PUNCT_ love swim _IS_PUNCT_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RelationRecord(items=frozenset({' '}), support=0.15873015873015872, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({' '}), confidence=0.15873015873015872, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'e'}), support=0.15873015873015872, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'e'}), confidence=0.15873015873015872, lift=1.0)])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "text = post_test['title'][0]\n",
    "\n",
    "results = list(apriori(text))\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
