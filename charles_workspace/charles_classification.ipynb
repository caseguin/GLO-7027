{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDENTITY_ATTACK</th>\n",
       "      <th>INSULT</th>\n",
       "      <th>PROFANITY</th>\n",
       "      <th>SEVERE_TOXICITY</th>\n",
       "      <th>THREAT</th>\n",
       "      <th>TOXICITY</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>message_comments</th>\n",
       "      <th>mainTopic</th>\n",
       "      <th>message_posts</th>\n",
       "      <th>secondTopic</th>\n",
       "      <th>shares</th>\n",
       "      <th>title</th>\n",
       "      <th>time_difference</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102216</td>\n",
       "      <td>0.651296</td>\n",
       "      <td>0.664565</td>\n",
       "      <td>0.350583</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.588517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Faux, ce ne sera jamais le temps pour lui car ...</td>\n",
       "      <td>composer-preview</td>\n",
       "      <td>HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...</td>\n",
       "      <td>7FJ4TUHKEFEXFIZI6DY2WAQE4E</td>\n",
       "      <td>0</td>\n",
       "      <td>Les incohérences de l’ingérence</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.167515</td>\n",
       "      <td>0.255499</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>0.176317</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon dieu!! On nous prend sérieusement pour des...</td>\n",
       "      <td>composer-preview</td>\n",
       "      <td>HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...</td>\n",
       "      <td>7FJ4TUHKEFEXFIZI6DY2WAQE4E</td>\n",
       "      <td>0</td>\n",
       "      <td>Les incohérences de l’ingérence</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDENTITY_ATTACK    INSULT  PROFANITY  SEVERE_TOXICITY    THREAT  TOXICITY  \\\n",
       "0         0.102216  0.651296   0.664565         0.350583  0.029933  0.588517   \n",
       "1         0.005476  0.167515   0.255499         0.006981  0.005647  0.176317   \n",
       "\n",
       "   comment_count  like_count  \\\n",
       "0              0           0   \n",
       "1              1           1   \n",
       "\n",
       "                                    message_comments         mainTopic  \\\n",
       "0  Faux, ce ne sera jamais le temps pour lui car ...  composer-preview   \n",
       "1  Mon dieu!! On nous prend sérieusement pour des...  composer-preview   \n",
       "\n",
       "                                       message_posts  \\\n",
       "0  HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...   \n",
       "1  HÉLÈNE BUZZETTI / Jagmeet Singh a soutenu que ...   \n",
       "\n",
       "                  secondTopic  shares                            title  \\\n",
       "0  7FJ4TUHKEFEXFIZI6DY2WAQE4E       0  Les incohérences de l’ingérence   \n",
       "1  7FJ4TUHKEFEXFIZI6DY2WAQE4E       0  Les incohérences de l’ingérence   \n",
       "\n",
       "   time_difference  year  month  weekday  \n",
       "0              3.0  2023      6        4  \n",
       "1             18.0  2023      6        4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== Importer les library ====== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ====== Déterminer les path ====== #\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = os.path.dirname(cwd)\n",
    "data_path = parent + '\\\\data\\\\'\n",
    "\n",
    "df =pd.read_hdf(cwd + '\\\\dataframe\\\\' + 'df_clean.h5')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(data_path + 'Posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Une électrification à « la mesure des besoins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les incohérences de l’ingérence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L’état d’urgence décrété à Sept-Îles en raison...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Le Petit Champlain en chantier, des commerçant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legault veut empêcher les antiavortement de fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  Une électrification à « la mesure des besoins ...\n",
       "1                    Les incohérences de l’ingérence\n",
       "2  L’état d’urgence décrété à Sept-Îles en raison...\n",
       "3  Le Petit Champlain en chantier, des commerçant...\n",
       "4  Legault veut empêcher les antiavortement de fa..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = posts[['title']]\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_test = posts.head(100).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_secondtopics = ['actualités', 'politique', 'sports', 'divertissement', 'santé', 'éducation',\n",
    "                            'hockey', 'musique', 'auto', 'affaires-locales', 'environnement', 'covid-19',\n",
    "                            'science', 'voyages', 'mode', 'arts', 'alimentation', 'cinéma', 'technologie',\n",
    "                            'culture', 'jeunesse', 'expositions', 'techno',\n",
    "                            'théâtre', 'livres', 'voyages', 'recettes', 'arts-visuels', 'sexe', 'estrie',\n",
    "                            'remparts', 'sciences', 'société', 'oceanic', 'vin', 'guerre',\n",
    "                            'canadiens','celebrites']\n",
    "\n",
    "relevant_maintopics = ['actualites', 'affaires', 'arts', 'monde', 'sports', 'vivre', 'chroniques', 'opinions',\n",
    "                   'consommation', 'jeunesse', 'magazines', 'actualite', 'le-mag', 'science', 'cinema', 'auto',\n",
    "                   'maison', 'covid-19', 'dossiers', 'programmation', 'quebec-billets', 'hockey-junior', \n",
    "                   'nos-recommandations', 'remparts', 'ma-region', 'events', 'protegez-vous', 'tokyo-2021', \n",
    "                   'lesasdelinfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_secondtopics = ['actualités', 'politique', 'sports', 'divertissement', 'santé', 'éducation',\n",
    "                         'hockey', 'musique', 'auto', 'affaires-locales', 'environnement', 'covid-19',\n",
    "                         'science', 'voyages', 'mode', 'arts', 'alimentation', 'cinéma', 'technologie',\n",
    "                         'culture', 'jeunesse', 'expositions', 'techno', 'théâtre', 'livres', 'voyages',\n",
    "                         'recettes', 'arts-visuels', 'sexe', 'estrie', 'remparts', 'sciences', 'société',\n",
    "                         'oceanic', 'vin', 'guerre', 'canadiens', 'celebrites']\n",
    "\n",
    "# Transform list to dictionary\n",
    "relevant_secondtopics_dict = {index: topic for index, topic in enumerate(relevant_secondtopics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nCamembertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 24\u001b[0m\n\u001b[0;32m     18\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mfloat\u001b[39m(i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m padded_tokens]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CamembertTokenizer, CamembertForSequenceClassification, AdamW\n\u001b[1;32m---> 24\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCamembertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m CamembertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1251\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1251\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1239\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1237\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nCamembertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Préparer les inputs\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokens = post_test['title'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True, padding='max_length'))\n",
    "\n",
    "# Pad the sequences\n",
    "max_len = max([len(token) for token in tokens])\n",
    "padded_tokens = pad_sequences(tokens, maxlen=max_len, dtype='long', value=0, truncating='post', padding='post')\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = [[float(i != 0.0) for i in token] for token in padded_tokens]\n",
    "\n",
    "\n",
    "\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, AdamW\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "# Convert labels to numerical format (if necessary)\n",
    "labels = [relevant_secondtopics_dict[label] for label in post_test['label']]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "input_ids = torch.tensor(padded_tokens)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Forward pass\n",
    "inputs = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_masks,\n",
    "    'labels': labels\n",
    "}\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nCamembertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_ids, attention_masks, labels\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCamembertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m CamembertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1251\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1251\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1239\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1237\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nCamembertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, CamembertTokenizer, CamembertForSequenceClassification, AdamW\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to tokenize, pad, and convert data to PyTorch tensors\n",
    "def preprocess_data(data, tokenizer, max_length):\n",
    "    # Tokenize the text data\n",
    "    tokens = data['title'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length'))\n",
    "    \n",
    "    # Pad the sequences\n",
    "    padded_tokens = pad_sequences(tokens, maxlen=max_length, dtype='long', value=0, truncating='post', padding='post')\n",
    "    \n",
    "    # Create attention masks\n",
    "    attention_masks = [[float(i != 0.0) for i in token] for token in padded_tokens]\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    input_ids = torch.tensor(padded_tokens)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor([relevant_secondtopics_dict[label] for label in data['label']])\n",
    "    \n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Preprocess the data\n",
    "input_ids, attention_masks, labels = preprocess_data(post_test, tokenizer, max_length=512)\n",
    "\n",
    "# Set the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Forward pass\n",
    "inputs = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_masks,\n",
    "    'labels': labels\n",
    "}\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
